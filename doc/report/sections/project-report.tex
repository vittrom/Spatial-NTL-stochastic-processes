% !TEX root = ../main.tex

% Project report section

\section{Project report}\label{project_report}

In this section we follow the analysis of \cite{james2006poisson} to derive a posterior distribution for the SPNTL process. In \Cref{NTL_definition} we give a rigorous definition of a NTL process and derive a formula of the random distribution function in terms of a stochastic process, similarly to Thm. 3.1 of \cite{doksum1974tailfree}. \Cref{CBM} introduces the cumulative birth measure, the random measure we will use to specify a SPNTL process. \Cref{SPNTL_process} outlines the SPNTL process and in \Cref{SPNTL_posterior} we detail the posterior analysis and derive the main result of the paper using results from Poisson partition calculus \cite{james2005poisson,james2006poisson}.

\subsection{Neutral to the left stochastic process}\label{NTL_definition}
The Bayesian analysis of SPNTL is based on NTL stochastic processes. \cite{doksum1974tailfree} provides a framework to analyse NTR processes and characterize the random distribution function in terms of a L{\'e}vy process. The NTR characterization, as well as the relation between the random distribution function and a L{\'e}vy process is essential to obtain the results in \cite{james2006poisson}. Therefore, we give here a similar characterization for NTL processes as this will aid the posterior analysis of SPNTL.

Given a sequence of random variables $X = (X_1, \ldots, X_k)$, we say that $X$ is NTL if the increments form a sequence, $R$, of mutually independent random variables in $[0, 1]$ \cite{bloem2018sampling}, that is

\begin{equation*}
R = \left(1, \frac{X_2}{X_1 + X_2}, \ldots, \frac{X_j}{\sum_{i=1}^j X_i}, \ldots, \frac{X_k}{\sum_{i=1}^k X_i}\right)
\end{equation*}
is a vector of mutually independent random variables on $[0,1]$. Let $V_1, \ldots, V_k$ be nonnegative independent random variables in $[0, 1]$ then $R$ can be defined as
\begin{equation*}
R = \left(1, V_2, \ldots, V_k\right)
\end{equation*}
and 
\begin{equation}\label{increments_v}
\begin{split}
\left(1, \frac{X_2}{X_1 + X_2}, \ldots, \frac{X_j}{\sum_{i=1}^j X_i}, \ldots, \frac{X_k}{\sum_{i=1}^k X_i}\right) =_d \left(1, V_2, \ldots,V_j, \ldots, V_k\right)
\end{split}
\end{equation}
where $=_d$ indicates equality in distribution a.s. From \Cref{increments_v} it is possible to derive the distribution of $X$ in terms of $V = (V_1, \ldots, V_k)$ as an "inverse" stick-breaking process like in \cite{bloem2017preferential}.
\begin{equation*}
\begin{split}
(X_1, X_2, \ldots,X_j, \ldots X_k) &=_d \left(\prod_{i=2}^k(1-V_i), V_2\prod_{i=3}^k(1-V_i), \ldots, V_j\prod_{i=j+1}^k(1-V_i), \ldots, V_k\right)
\end{split}
\end{equation*}
where we set $V_1 = 1$.

The increments in \Cref{increments_v} can be written equivalently in terms of a random distribution function $F$ corresponding to a random probability $P$ such that $F(t) = P([-\infty, t]) \in [0, 1]$ for $t \in \bbR_{+}$. The random distribution function is a stochastic process that satisfies

\begin{enumerate}
	\item $F$ is a.s. non-decreasing
	\item $\lim_{t \rightarrow - \infty} F(t) = 0$ a.s., $\lim_{t \rightarrow \infty} F(t) = 1$ a.s, and
	\item $\lim_{s \rightarrow t^+} F(s) = F(t)$ for each $t \in \bbR_{+}$
\end{enumerate}
as stated in \cite{doksum1974tailfree}.

The increments in \Cref{increments_v} can be rewritten in terms of the stochastic process $F$ as

\begin{equation}\label{increments_NTL_F}
\left(\frac{F(t_1)}{F(t_1)}, \frac{F(t_2) - F(t_1)}{F(t_2)}, \ldots, \frac{F(t_k) - F(t_{k-1})}{F(t_{k})} \right) =_d \left(1, V_2, \ldots, V_k\right)
\end{equation}
for $0 < t_1 < t_2 < \ldots < t_k < \infty$. The random distribution $F$ is essentially a random cumulative density function (CDF) and its stochastic process corresponds to a special type of L{\'e}vy process called \textit{subordinator} \cite{orbanz2012lecture}. 

\paragraph{L{\'e}vy process} \cite{papapantoleon2008introduction} A stochastic process $Z = \{Z(t) : t \in \bbR_+\}$ is said to be a L{\'e}vy process if it satisfies the following properties:
	\begin{enumerate}
		\item $Z(0) = 0$
		\item For any $0 < t_1 < t_2 < \ldots < t_k < \infty$, $Z(t_2) - Z(t_1), \ldots, Z(t_k) - Z(t_{k-1})$ are independent.
		\item For any $s < t$, $Z(t) - Z(s) =_d Z(t-s)$
		\item For any $\epsilon> 0$ and $t \in \bbR_{+}$, $\lim_{s \rightarrow t}P( |Z(t)-Z(s) > \epsilon|) = 0$
	\end{enumerate}

Given the correspondence between the random CDF and a L{\'e}vy process, we can find a function of a L{\'e}vy process, $Z$, such that the random distribution $F$ satisfies the NTL properties. The need to express $F$ as a function of $Z$ is motivated by the fact that working with a L{\'e}vy process simplifies some derivations. In Theorem 1 we define $F$ as a function of a L{\'e}vy process for NTL process, the analogous of Theorem 1 for NTR processes can be found in \cite{doksum1974tailfree}.

\paragraph{Theorem 1} $F_{t_k}(t)$ is a random distribution function neutral-to-the-left if and only if it has the same probability distribution as
\begin{equation}\label{NTL_eq}
e^{Z(t) - Z(t_k)}, \quad t \in (0, t_k]
\end{equation}
for some L{\'e}vy process $Z$.

\textit{Proof:} We start by proving that by writing $F_{t_k}$ as in \Cref{NTL_eq} we obtain independent increments. For any increment $I[t_{j-1}, t_j]$ of a NTL process,
\begin{equation}\label{jump_equation}
\begin{split}
\frac{F_{t_k}(t_j) - F_{t_k}(t_{j-1})}{F(t_j)} = 1 - \frac{F_{t_k}(t_{j-1})}{F_{t_k}(t_j)} = 1 - e^{Z(t_{j-1}) -Z(t_j)}
\end{split}
\end{equation}
since increments of a L{\'e}vy process are independent, it follows that increments of a NTL generated by \Cref{NTL_eq} are independent. 

We now show that the process, $Z(t)$, following from the definition of $F$ has independent increments, hence proving it is a L{\'e}vy process. We first write a general expression for $F_{t_k}(t_j)$ derived from \Cref{increments_NTL_F}. First note that
\begin{equation*}
\begin{split}
&\frac{F_{t_k}(t_k) - F_{t_k}(t_{k-1})}{F_{t_k}(t_{k})} = V_k \implies F_{t_k}(t_{k-1}) = (1-V_k)
\end{split}
\end{equation*} 
recursively we obtain
\begin{equation}\label{NTL_F}
F_{t_k}(t_j) = \prod_{i=j+1}^k (1 - V_i)
\end{equation}
The increments of the stochastic process $Z(t)$ can be written as 
\begin{equation*}
\begin{split}
Z(t_j) - Z(t_{j-1}) &= \log(F_{t_k}(t_j)) - \log(F_{t_k}(t_{j-1}))  = \log\left(\frac{F_{t_k}(t_{j})}{F_{t_k}(t_{j-1})}\right) \\ &= \log\left(\frac{\prod_{i=2}^{j-1} (1 - V_i)}{\prod_{i=2}^{j} (1 - V_i)}\right) = -\log(1 - V_j)
\end{split}
\end{equation*}
hence showing that the increment at time $t_j$ only depends on a function of $V_j$ and since $V_i$ for $i=1, \ldots, k$ are independent, the increments of $Z(t)$ are independent. This shows that $Z(t)$ is a L{\'e}vy process. $\qed$

Note that it is important to define $F$ on a specific interval to ensure it represents a proper CDF. However, a fixed time interval is by no mean restrictive and the results derived in the following hold for any time interval. In fact, the random CDF in \Cref{NTL_eq} can be normalized to any interval as
\begin{equation*}
F_{t_k}(t) = \frac{F_{t_{k-1}}}{e^{Z(t_k) - Z(t_{k-1})}} \quad t \in (0, t_k]
\end{equation*}
The definition of $F$ represents a stochastic process on $\bbR_{+} \times \borel(\bbR_{+})$ and is valid for any time interval. Hence, to simplify notation in the rest of the analysis we omit the subscript of $F$ unless necessary.

\subsection{Cumulative birth measure}\label{CBM}
To analyse NTR processes \cite{james2006poisson} uses the approach of \cite{hjort1990nonparametric} and works with the cumulative hazard measure $\Lambda$, a completely random measure that directly relates to the NTR increments. For NTL processes we define a similar object and name it cumulative birth measure 
\begin{equation}\label{birth_measure}
B(ds) = F_{t_k}(ds)/F_{t_k}(s+), \quad s \in (0, t_k]
\end{equation}
where $ds = (s+) - (s-)$ and $F_{t_k}(s+) = P(T \leq s+)$ with $T$ a random variable following the distribution $F$. $B$ is a completely random measure. Using the cumulative birth measure we can write
\begin{equation*}
P(T \in ds | F) = F(ds) = B(ds)F(s+) = B(ds)e^{Z(s+) - Z(t_k)}
\end{equation*}
Defining the cumulative birth measure as in \Cref{birth_measure} lets us relate random jumps in the L{\'e}vy process $Z$ to random jumps in $B$ in exactly the same way as done in \cite{james2006poisson} for $Z$ and $\Lambda$. 

\paragraph{Proposition 1} A random jump $J_j$ in $B$ taking values in $[0,1]$ corresponds to a random jump $-\log(1 - J_j)$ in $Z$ taking values in $\bbR_+$.

\textit{Proof:} The proof is simple and follows equating $B$ and $J_j$ and using \Cref{jump_equation}. Let $t_j$ and $t_{j-1}$ denote the interval of the jump $J_j$ and denote $dj = t_j - t_{j-1}$ then
\begin{equation*}
\begin{split}
B(dj) = \frac{F(dj)}{F(t_j)} = 1 - e^{Z(t_{j-1}) -Z(t_j)} = J_j \implies Z(t_j) - Z(t_{j-1}) = -\log(1- J_j) \qed
\end{split}
\end{equation*}
If we let $\tau$ describe the conditional distribution of the jumps of $Z$ and $\rho$ the conditional distribution of the jumps of $B$, Proposition 1 implies the following relationship between $\tau$ and $\rho$.
\begin{equation*}
\begin{split}
&\tau(y|s) = e^{-y}\rho\left(1- e^{-y} | s\right) \ \ \qquad\qquad y \in \bbR_{+} \\ & \rho(u|s) = (1- u)\tau\left(-\log(1-u) |s\right) \quad u \in [0, 1]
\end{split}
\end{equation*}



\subsection{Spatial neutral to the left process}\label{SPNTL_process}
The previous sections provided the background necessary to define the SPNTL process. Since our work extends \cite{james2006poisson}, we borrowed James' approach and notation in several parts. 

The SPNTL process is motivated by the observation that the NTL process does not allow inference on spaces more complex than the real line however, complex spaces often appear in practice. In order to work with more complex spaces, we extend the space on which the random distribution $F$ is defined to an arbitrary Polish space $\calS = \bbR_{+} \times \borel(\bbR_{+}) \times \calX$ with $\calX$ an arbitrary Polish space. We denote by $(R, T, X)$ the elements of the Polish space $\calS$ that have distribution $F_r(ds, dx)$ for $(r, s, x) \in \cal{S}$. The extension to the Polish space $\calS$ is such that $F_r(ds, \calX)$ is a NTL process. To avoid carrying over the subscript in $F$, in the rest of the analysis we assume $r$ fixed and deal only with $(s, x) \in \calS_{-r}$.

In his analysis of SPNTR \cite{james2006poisson} used results from Poisson partition calculus. This, significantly simplified proofs as combinatorial arguments, a common but challenging approach for similar proofs (see e.g. \cite{antoniak1974mixtures,pitman2002combinatorial}), could be avoided. To apply Poisson calculus for SPNTL processes, we extend $Z$ and $B$ to completely random measures on $\calS_{-r}$ using a representation in terms of a Poisson random measure, similarly to \cite{james2006poisson}. Let $N$ denote a Poisson random measure on the Polish space $\calW = [0,1] \times \calS_{-r}$ with mean intensity 
\begin{equation*}
E[N(du, ds, dx) | \nu] = \nu(du, ds, dx) = \rho(du | s)B_0(ds, dx)
\end{equation*}
where $\rho$ satisfies $\int_0^1 u \rho(du) = 1$ as in \cite{james2006poisson} and $B_0(ds, dx) = F_0(ds, dx)/F_0(s+)$ is the birth measure on $\calS_{-r}$ with $F_0$ denoting a prior on $F$ in $\calS_{-r}$ and $F_0(s+) = F_0(s+, \calX)$.

Moreover, we denote the law of $N$ with intensity $\nu$ as $P(dN|\nu)$ and the Laplace functional of $N$ as
\begin{equation*}
E\left[e^{-N(f)}|\nu\right] = \int_{\bbM} e^{-N(f)}P(dN|\nu) = e^{-\calG(f)}
\end{equation*}
where for any positive $f$, $N(f) = \int_{\calW} f(x)N(dx)$ and $\calG(f) =\int_{\calW}(1-e^{-f(x)})\nu(dx)$ and $\bbM$ is the space of boundedly finite measures on $\cal{W}$. The definitions above are the same as those in \cite{james2006poisson}, however, we presented them here since they will be necessary for the rest of the analysis.

From the specifications above we have that $B(ds, dx) = \int_0^1 uN(du,ds,dx)$ is a completely random measure on $\calS_{-r}$ with $E\left[B(ds, dx)\right] = B_0(ds,dx)$ and the corresponding for the L{\'e}vy process $Z$, $Z(ds, dx) = \int_0^1 [-\log(1-u)]N(du,ds,dx)$. Furthermore, we define $Z(t+) = \int_{\calW}[-\indicator(s < t)\log(1-u)]N(du,ds,dx)$. Using these definitions we can specify a SPNTL random probability measure on $\calS_{-r}$ as 
\begin{equation*}
P(T \in dt, X \in dx | F) = F(dt, dx) = F(t+)B(dt, dx)  
\end{equation*}
and $E\left[F(dt, dx)\right] = F_0(dt, dx)$. \cite{james2006poisson} also covers the case of prior fixed discontinuity points; with proper modifications such extension can be included in this work however, to keep the analysis concise we do not cover this case here.

\subsection{Posterior analysis}\label{SPNTL_posterior}
In this section we perform the posterior analysis for SPNTL processes. We first introduce the necessary notation in \Cref{notation} and then in \Cref{SPNTL_posterior_proof} characterize the posterior distribution of SPNTL processes similarly to Proposition 4.1 of \cite{james2006poisson}.

\subsubsection{Notation}\label{notation}
Let $(T_i, X_i)|F$ for $i=1, \ldots, n$ be i.i.d. pairs from $F$ with $F$ modeled as a SPNTL. We aim to find the posterior distribution of $F|\boldsymbol{T}, \boldsymbol{X}$ and the marginal distribution of $(\boldsymbol{T}, \boldsymbol{X}) = \{(T_1, X_1), \ldots, (T_n, X_n)\}$. Since $F$, $B$ and $Z$ are functionals of $N$, we work with the joint distribution of $(\boldsymbol{T}, \boldsymbol{X}, N)$ given by

\begin{equation}\label{joint_dist}
\left[\prod_{i=1}^n F(T_i +) B(dT_i, dX_i)\right]P(dN|\nu) = \pi(dN|\boldsymbol{T}, \boldsymbol{X}) \calM(dT_1, dX_1, \ldots, dT_n, dX_n)
\end{equation}
where $\pi(dN|\boldsymbol{T}, \boldsymbol{X})$ is the posterior distribution of $N|\boldsymbol{T}, \boldsymbol{X}$ and 

\begin{equation*}
\calM(dT_1, dX_1, \ldots, dT_n, dX_n) = \calM(d\boldsymbol{T}, d\boldsymbol{X}) = \int_{\calM}\left[\prod_{i=1}^n F(dT_i, dX_i)\right]P(dN|\nu)
\end{equation*}
the exchangeable marginal distribution of $(\boldsymbol{T}, \boldsymbol{X})$.

The distribution of $(\boldsymbol{T}, \boldsymbol{X})$ can be equally represented by the distribution of $(O(\boldsymbol{T}^*), \boldsymbol{X}^*, \boldsymbol{m})$ where $(\boldsymbol{T}^*, \boldsymbol{X}^*)$ are the unique pairs of $(\boldsymbol{T}, \boldsymbol{X})$, $O(\cdot)$ denotes the set ordering operator and $\boldsymbol{m}$ is the set indicating the partitions of $(\boldsymbol{T}, \boldsymbol{X})$ into the unique ordered values $(O(\boldsymbol{T}^*), \boldsymbol{X}^*)$. In particular, let $T_{(1:n)} > T_{(2:n)} > \ldots > T_{(n(\boldsymbol{p}):n)}$ be the $n(\boldsymbol{p}) \leq n$ ordered times with $X_j^*$ for $j=1, \ldots, n(\boldsymbol{p})$ being the corresponding unique value associated to them, then $\boldsymbol{m} = \{E_{(1)}, \ldots, E_{(n(\boldsymbol{p}))}\}$ where $E_{(j)} = \{i : T_i = T_{(j:n)}\}$ for $j=1, \ldots, n(\boldsymbol{p})$, $m_j = |E_{(j)}|$ and $\sum_{j=1}^{n(\boldsymbol{p})} m_j = n$. 

Since the posterior distribution of a NTL process does not only depend on $m_j$, the number of births at time $T_{(j:n)}$, but also depends on the previous births, we define the number of previous births as
\begin{equation*}
Y_n(s) = \sum_{i=1}^n \indicator(T_i < s) = \sum_{l=1}^{n(\boldsymbol{p})} m_l \indicator(T_{(l:n)} < s)
\end{equation*}
For $j=1, \ldots, n(\boldsymbol{p})$, we define $r_j = Y_n(T_{(j:n)}) = \sum_{l=j+1}^{n(\boldsymbol{p})} m_l$ to denote the total number of births before time $T_{(j:n)}$. Note that $r_0 = n$ and $r_{n(\boldsymbol{p})} = 0$.

\subsubsection{Posterior distribution of SPNTL processes} \label{SPNTL_posterior_proof}
Let $\boldsymbol{W} = (\boldsymbol{J}, \boldsymbol{T}, \boldsymbol{X})$ with $\boldsymbol{J} = \{J_1, \ldots, J_n\}$ the collection of latent jumps in $[0,1]$ and $W_i = (J_i, T_i, X_i)$ for $i=1, \ldots, n$ elements of $\calW$. Furthermore, let $J_{j,n}$ be the unique jumps in $\boldsymbol{J}$  and denote by $W_j^* = (J_{j,n}, T_{(j:n)}, X_j^*)$, $j=1, \ldots, n(\boldsymbol{p})$ the unique triples. The idea is to work with the distribution of $(\boldsymbol{W}, N)$ as all posterior distributions can then be recovered integrating out the additional elements. Given the equivalence in distribution with the set of ordered values, to simplify the analysis, we decide to work with the ordered values.

The distribution of $(\boldsymbol{W}, N)$ is given in Proposition 2.3 of \cite{james2005poisson} as
\begin{equation}\label{james_poisson_joint}
\left[\prod_{i=1}^n N(dW_i)\right]e^{-N(f)}P(dN|\nu) = e^{-\mathcal{G}(f)}P(dN|\nu_f, \boldsymbol{W}) \prod_{j=1}^{n(\boldsymbol{p})} e^{-f(W_j^*)}\nu(dW_j^*)
\end{equation}
where $P(dN|\nu_f, \boldsymbol{W})$ is the law of the random measure $N + \sum_{j=1}^{n(\boldsymbol{p})} \delta_{W_j^*}$ with $N$ a Poisson random measure with mean intensity $E[N(du, ds, dx)|\nu_f] = \nu_f(du, ds,dx) = e^{-f(u,s,x)}\nu(du,ds,dx)$ and  $f$ any nonnegative measurable function such that $\mathcal{G}(f) < \infty$.

To use the result above, we need to be able to write the distribution of $(\boldsymbol{W}, N)$ as a function of $N$, the Poisson random measure. More specifically, the object that needs to be rewritten as a function of $N$ is
\begin{equation}\label{product_f}
\prod_{i=1}^n F\left(T_i + \right) = \prod_{j=1}^{n(\boldsymbol{p})} F\left(T_{(j:n)} + \right)^{m_j} = e^{-nZ(T_{(1:n)})} \prod_{j=1}^{n(\boldsymbol{p})} e^{m_j Z(T_{(j:n)})}
\end{equation}
of \Cref{joint_dist} where the time subscript of $F$ corresponds to $T_{(1:n)}$ as this is the most recent time in the sequence. Note that only the product in \Cref{product_f} needs to be rewritted as $B(dt, dx)$ is, by definition, a function of $N$. Our goal is to find a nonnegative function $f$ such that \Cref{product_f} can be expressed as $e^{-N(f)}$.

Let $g_{T_{(j:n)}}(u,s,x) = \indicator\left(s < T_{(j:n)}\right)[-\log(1-u)]$ and define 
\begin{equation*}
\begin{split}
g_n(u,s,x) &= \sum_{j=1}^{n(\boldsymbol{p})} m_j g_{T_{(j:n)}}(u,s,x) = \sum_{j=1}^{n(\boldsymbol{p})} m_j \left(1 - \indicator\left(s > T_{(j:n)}\right)\right)[-\log(1-u)]\\ &= n[-\log(1-u)] - Y_n(s)[-\log(1-u)] = \left(n- Y_n(s)\right)[-\log(1-u)]
\end{split}
\end{equation*}

Rewriting \Cref{product_f} as a function of the Poisson random measure $N$ and of $g$ we obtain
\begin{equation*}
e^{-nZ(T_{(1:n)}} \prod_{j=1}^{n(\boldsymbol{p})} e^{m_j Z(T_{(j:n)})} = e^{-N\left(ng_{T_{(1:n)}}\right)} e^{N(g_n)}  = e^{-N\left(ng_{T_{(1:n)}} - g_n\right)} = e^{-N(f_n)}
\end{equation*}
with
\begin{equation*}
f_n(u,s,x) = ng_{T_{(1:n)}}(u,s,x) - g_n(u,s,x) = [-\log(1-u)]\left[Y_n(s) - n\indicator\left(s > T_{(1:n)}\right)\right] \geq 0.
\end{equation*}
The function $f$ allows to rewrite the product in \Cref{product_f} as a function of the Poisson random measure $N$  with mean intensity

\begin{equation*}
E[N(du, ds, dx)|\nu_f] = \nu_f(du, ds,dx) = e^{-f(u,s,x)}\nu(du,ds,dx) = (1 - u) ^ {Y_n(s) - n\indicator\left(s > T_{(1:n)}\right)} \nu(du,ds,dx)
\end{equation*}
Furthermore, note that $e^{-f_n(J_{j,n},T_{(j:n)},X_j^*)} = (1 - J_{j,n}) ^ {r_j}$. Applying \Cref{james_poisson_joint} we can obtain the joint distribution of $(\boldsymbol{J}, \boldsymbol{T}, \boldsymbol{X}, N)$ like in Equation 18 of \cite{james2006poisson}

\begin{equation}
P(dN|e^{-f_n}\nu_{f_n}, \boldsymbol{W})e^{-\mathcal{G}(f_n)}\left[\prod_{j=1}^{n(\boldsymbol{p})} J_{j,n}^{m_j}(1- J_{j,n})^{r_j} \rho(dJ_{j,n}|T_{(j:n)})\right]\prod_{l=1}^{n(\boldsymbol{p})}B_0(T_l^*, X_l^*)
\end{equation}

From this result, following \cite{james2006poisson}, we can derive $\calM(d\boldsymbol{T}, d\boldsymbol{X})$ integrating out $N$ and the collection $J_{j,n}$ as

\begin{equation*}
\calM(d\boldsymbol{T}, d\boldsymbol{X}) = e^{-\mathcal{G}(f_n)}\left[\prod_{j=1}^{n(\boldsymbol{p})} \kappa_{m_j, r_j}\left(\rho| T_{(j:n)}\right)\right]\prod_{l=1}^{n(\boldsymbol{p})}B_0(T_l^*, X_l^*)
\end{equation*}
with 
\begin{equation*}
\kappa_{m_j, r_j}\left(\rho |T_{(j:n)}\right)=\int_0^1 J_{j,n}^{m_j}\left(1- J_{j,n}\right)^{r_j} \rho\left(dJ_{j,n}|T_{(j:n)}\right)
\end{equation*}
Moreover, we can characterize the posterior of a SPNTL process similarly to Proposition 4.1 in \cite{james2006poisson}.

\paragraph{Theorem 2} Let $F$ be a SPNTL process defined by the Poisson random measure $N$ with mean intensity $\nu(du, ds, dx) = \rho(du | s)B_0(ds, dx)$ and $B$ its corresponding cumulative birth measure. Moreover, let $(T_i, X_i)|F$ be i.i.d. from $F$ for $i = 1, \ldots, n$. Then:
\begin{enumerate}
	\item The posterior distribution of $N|\boldsymbol{T}, \boldsymbol{X}$ is equivalent to the distribution of $N_n^* = N_n + \sum_{j=1}^{n(\boldsymbol{p})} \delta_{J_{j,n}, T_{(j:n)}, X_j^*}$ where conditional on $(\boldsymbol{T}, \boldsymbol{X})$, $N_n$ is a Poisson random measure with intensity
	\begin{equation*}
	\nu_n(du,ds,dx) = (1-u)^{Y_n(s) - n\indicator\left(s > T_{(1:n)}\right)}\rho(du|s)B_0(ds, dx)
	\end{equation*}
	Moreover, $J_{j,n}$ for $j=1, \ldots, n(\boldsymbol{p})$ are conditionally independent of $N_n$ and mutually independent with distribution
	\begin{equation*}
	P\left(J_{j,n} \in du | T_{(j:n)}\right) \propto u^{m_j}(1-u)^{r_j}\rho\left(du|T_{(j:n)}\right) \quad j=1, \ldots, n(\boldsymbol{p})
	\end{equation*}
	\item The posterior distribution of $B$ given $(\boldsymbol{T}, \boldsymbol{X})$ is equivalent to 
	\begin{equation*}
	B_n^*(ds, dx) = \int_0^1 u N_n^*(du,ds,dx) = B_n(ds,dx) + \sum_{j=1}^{n(\boldsymbol{p})} J_{j,n} \delta_{T_{(j:n)}, X_j^*}(ds,dx)
	\end{equation*}
	where $B_n(ds,dx) = \int_0^1 u N_n(du,ds,dx)$.
	\item The posterior of the $Z$ process is 
	\begin{equation*}
	Z_n^*(ds, dx) = Z_n(ds,dx) + \sum_{j=1}^{n(\boldsymbol{p})} Z_{j,n} \delta_{T_{(j:n)}, X_j^*}(ds,dx)
	\end{equation*}
	where $Z_n(ds,dx) = \int_0^1 [-\log(1-u)] N_n(du,ds,dx)$ and $Z_{j,n} = -\log(1-J_{j,n})$ with
	\begin{equation*}
	P(Z_{j,n} \in dy|T_{(j:n)}) \propto  (1-e^{-y})^{m_j}e^{-r_j}\tau\left(dy|T_{(j:n)}\right)
	\end{equation*}
	\item The posterior distribution of $F$ is given by
	\begin{equation*}
	e^{Z_n(s+) - Z_n\left(T_{(1:n)}\right)}\left[\prod_{j: T_{(j:n)} < s}^{n(\boldsymbol{p})} (1 - J_{j,n})\right] B_n(ds, dx) + \sum_{j=1}^{n(\boldsymbol{p})} \tilde{P}_{j,n}\delta_{T_{(j:n)}, X_j^*}(ds,dx)
	\end{equation*}
	where $\tilde{P}_{j,n} = e^{Z_n(T_{(j:n)}+) - Z_n\left(T_{(1:n)}\right)} J_{j,n}\prod_{l=j+1}^{n(\boldsymbol{p})}(1- J_{l,n})$.
\end{enumerate}